{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97646c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# P Naidoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf1fc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.5.4)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.13.3; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from opencv-python) (1.19.5)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.24.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease                \u001b[0m\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu bionic-backports InRelease              \u001b[0m\u001b[33m\n",
      "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1581 B]\n",
      "Hit:5 http://security.ubuntu.com/ubuntu bionic-security InRelease        \u001b[0m  \u001b[0m\u001b[33m\u001b[33m\u001b[33m\n",
      "Err:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Reading package lists... Done0m0m\u001b[33m\u001b[33m\n",
      "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is no longer signed.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "E: Invalid operation apt-get\n",
      "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1581 B]\n",
      "Err:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease                        \n",
      "Hit:3 http://security.ubuntu.com/ubuntu bionic-security InRelease   \n",
      "Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease          \n",
      "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Hit:7 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
      "Reading package lists... Done                      \n",
      "W: GPG error: https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY A4B469963BF863CC\n",
      "E: The repository 'https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease' is no longer signed.\n",
      "N: Updating from such a repository can't be done securely, and is therefore disabled by default.\n",
      "N: See apt-secure(8) manpage for repository creation and user configuration details.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libsm6 is already the newest version (2:1.2.2-1).\n",
      "libxext6 is already the newest version (2:1.3.3-1).\n",
      "ffmpeg is already the newest version (7:3.4.11-0ubuntu0.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 82 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install scipy\n",
    "!pip install pandas\n",
    "!pip install shutil\n",
    "!pip install opencv-python\n",
    "!pip install sklearn\n",
    "!apt update && apt install -y libsm6 libxext6\n",
    "!apt apt-get install -y libxrender-dev\n",
    "!apt-get update\n",
    "!apt-get install ffmpeg libsm6 libxext6  -y\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "\n",
    "LOCAL_DATA_PATH = '/tf/notebooks/Data/echonet frames split for model training'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3ba3ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = LOCAL_DATA_PATH\n",
    "train_dir = os.path.join(LOCAL_DATA_PATH, 'images/training')\n",
    "test_dir = os.path.join(LOCAL_DATA_PATH, 'images/testing')\n",
    "val_dir = os.path.join(LOCAL_DATA_PATH, 'images/validation')\n",
    "annot_dir = os.path.join(LOCAL_DATA_PATH, 'annotations_binary')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848f2178",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f277022",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OUTPUT_CHANNELS = 1 # no of classes\n",
    "BATCH_SIZE = 16      # 8 for GPU Server\n",
    "BUFFER_SIZE = 1000\n",
    "EPOCHS = 100         # 100 for GPU Server\n",
    "INPUT_IMAGE_SIZE_PIXELS = 512\n",
    "INPUT_SHAPE_IMAGE = [512, 512, 1]\n",
    "LEARNING_RATE=0.00001\n",
    "VAL_SUBSPLITS = 5\n",
    "\n",
    "NUM_OF_EXPERIMENTS = 1      #How many times to run each task\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 20 # 3 is the default value in tensorflow\n",
    "\n",
    "SEED = 24\n",
    "\n",
    "INITIAL_PERCENTAGE = 100\n",
    "\n",
    "MODEL_NAME = f'Model training on ECHONET with Tversky loss'\n",
    "\n",
    "# if WITH_DROPOUT:\n",
    "#     MODEL_NAME = f'{MODEL_NAME} with Dropout'\n",
    "    \n",
    "SAVE_FOLDER = 'Trained Models'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b342bfc",
   "metadata": {},
   "source": [
    "# Functions for reading images, displaying predictions, and callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48795e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_image(img_path: str) -> dict:\n",
    "    \"\"\"Load an image and its annotation (mask) and return a dictionary.\n",
    "\n",
    "    img_path : str : Image (not the mask) location.\n",
    "    dict: Dictionary mapping an image and its annotation.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_png(image, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "\n",
    "    # For one Image path:\n",
    "    # .../dataset/images/training/im_00000001.jpg\n",
    "    # Its corresponding annotation path is:\n",
    "    # .../dataset/annotations/training/im_00000001.png\n",
    "    mask_path = tf.strings.regex_replace(img_path, \"images\", \"annotations_binary\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"jpg\", \"png\")\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    # The masks contain a class index for each pixels\n",
    "    mask = tf.image.decode_png(mask, channels=1)\n",
    "    # In scene parsing, \"not labeled\" = 255\n",
    "    # But it will mess up with our N_CLASS = 150\n",
    "    # Since 255 means the 255th class, which doesn't exist\n",
    "    #mask = tf.where(mask == 255, np.dtype('uint8').type(0), mask)\n",
    "    # Note that we have to convert the new value (0)\n",
    "    # With the same dtype than the tensor itself    \n",
    "\n",
    "    return {'image': image, 'segmentation_mask': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb7221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(input_image, input_mask):\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0  \n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45c63dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(datapoint):\n",
    "  input_image = tf.image.resize(datapoint['image'], (INPUT_IMAGE_SIZE_PIXELS, INPUT_IMAGE_SIZE_PIXELS))\n",
    "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (INPUT_IMAGE_SIZE_PIXELS, INPUT_IMAGE_SIZE_PIXELS))\n",
    "\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51acd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize an image example and its corresponding \n",
    "#mask from the dataset.\n",
    "\n",
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a13a9e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  #pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  #pred_mask = pred_mask[..., tf.newaxis]\n",
    "  pred_mask = tf.greater(pred_mask, 0.5)\n",
    "  pred_mask = tf.dtypes.cast(pred_mask, tf.float32)\n",
    "  #pred_mask = pred_mask[..., tf.newaxis]\n",
    "  pred_mask = pred_mask[0]\n",
    "  return pred_mask\n",
    "\n",
    "def create_mask_temp(pred_mask):\n",
    "  #pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  #pred_mask = pred_mask[..., tf.newaxis]\n",
    "  pred_mask = tf.greater(pred_mask, 0.5)\n",
    "  pred_mask = tf.dtypes.cast(pred_mask, tf.float32)\n",
    "  #pred_mask = pred_mask[..., tf.newaxis]  \n",
    "  return pred_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "947d1d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=1):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    display([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f34e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask2(pred_mask):\n",
    "  #pred_mask = tf.argmax(pred_mask, axis=-1)\n",
    "  #pred_mask = pred_mask[..., tf.newaxis]\n",
    "  pred_mask = tf.greater(pred_mask, 0.5)\n",
    "  pred_mask = tf.dtypes.cast(pred_mask, tf.float32)\n",
    "  #pred_mask = pred_mask[..., tf.newaxis]\n",
    "  pred_mask = pred_mask[1]\n",
    "  return pred_mask\n",
    "\n",
    "def show_predictions_manual(image, mask, predicted): \n",
    "    display([image[0], mask[0], create_mask(predicted)])\n",
    "    \n",
    "def show_predictions_manual2(image, mask, predicted): \n",
    "    display([image[0], mask[0], create_mask2(predicted)])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4470242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    clear_output(wait=True)\n",
    "    show_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46ffc1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to call above functions:\n",
    "\n",
    "#for images, masks in train.take(2):\n",
    "#  sample_image, sample_mask = images, masks\n",
    "#  display([sample_image, sample_mask])\n",
    "\n",
    "\n",
    "#show_predictions(test_data, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6370e9f0",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b62a23ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from tensorflow.keras import backend as K \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#For plotting a model:\n",
    "#tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf92503",
   "metadata": {},
   "source": [
    "### Contracting (downward) path\n",
    "\n",
    "The (height, width, length) of the input gets smaller as you move down this path, and the number of channels increases.\n",
    "\n",
    "### Expanding (upward) Path\n",
    "\n",
    "The image's (height, width, length) all get larger in the expanding path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064466e1",
   "metadata": {},
   "source": [
    "The \"depth\" of your U-Net is equal to the number of down-convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a6a700",
   "metadata": {},
   "source": [
    "# Original UNET:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6be8c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.keras.initializers.HeNormal(seed=SEED)\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "#shape = (2, 2)\n",
    "#initializer = tf.initializers.he_normal(seed = SEED)\n",
    "#var = tf.Variable(initializer(shape=shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31efce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(input_tensor, num_filters):\n",
    "  encoder = tf.keras.layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
    "  encoder = tf.keras.layers.BatchNormalization()(encoder)\n",
    "  encoder = tf.keras.layers.Activation('relu')(encoder)\n",
    "  encoder = tf.keras.layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
    "  encoder = tf.keras.layers.BatchNormalization()(encoder)\n",
    "  encoder = tf.keras.layers.Activation('relu')(encoder)\n",
    "  return encoder\n",
    "\n",
    "def encoder_block(input_tensor, num_filters):\n",
    "  encoder = conv_block(input_tensor, num_filters)\n",
    "  encoder_pool = tf.keras.layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
    "  \n",
    "  return encoder_pool, encoder\n",
    "\n",
    "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
    "  decoder = tf.keras.layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
    "  decoder = tf.keras.layers.concatenate([concat_tensor, decoder], axis=-1)\n",
    "  decoder = tf.keras.layers.BatchNormalization()(decoder)\n",
    "  decoder = tf.keras.layers.Activation('relu')(decoder)\n",
    "  decoder = tf.keras.layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "  decoder = tf.keras.layers.BatchNormalization()(decoder)\n",
    "  decoder = tf.keras.layers.Activation('relu')(decoder)\n",
    "  decoder = tf.keras.layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
    "  decoder = tf.keras.layers.BatchNormalization()(decoder)\n",
    "  decoder = tf.keras.layers.Activation('relu')(decoder)\n",
    "  return decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b6eebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semantic loss functions initialized\n"
     ]
    }
   ],
   "source": [
    "from loss_functions import *\n",
    "loss_funcs = Semantic_loss_functions()\n",
    "\n",
    "def unet_model():\n",
    "    inputs = tf.keras.layers.Input(shape=INPUT_SHAPE_IMAGE)\n",
    "    encoder0_pool, encoder0 = encoder_block(inputs, 32)\n",
    "    encoder1_pool, encoder1 = encoder_block(encoder0_pool, 64)\n",
    "    encoder2_pool, encoder2 = encoder_block(encoder1_pool, 128)\n",
    "    encoder3_pool, encoder3 = encoder_block(encoder2_pool, 256)\n",
    "    encoder4_pool, encoder4 = encoder_block(encoder3_pool, 512)\n",
    "    center = conv_block(encoder4_pool, 1024)\n",
    "    decoder4 = decoder_block(center, encoder4, 512)\n",
    "    decoder3 = decoder_block(decoder4, encoder3, 256)\n",
    "    decoder2 = decoder_block(decoder3, encoder2, 128)\n",
    "    decoder1 = decoder_block(decoder2, encoder1, 64)\n",
    "    decoder0 = decoder_block(decoder1, encoder0, 32)    \n",
    "    outputs = tf.keras.layers.Conv2D(OUTPUT_CHANNELS, (1, 1), activation='sigmoid')(decoder0)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(optimizer=Adam(lr=LEARNING_RATE),\n",
    "                  loss=loss_funcs.tversky_loss,\n",
    "                  metrics=['accuracy'], run_eagerly=True\n",
    "                 )  \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f242a",
   "metadata": {},
   "source": [
    "# Prepare Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95705b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tf/notebooks/Data/echonet frames split for model training/images/training\n",
      "/tf/notebooks/Data/echonet frames split for model training/images/validation\n",
      "/tf/notebooks/Data/echonet frames split for model training/images/testing\n"
     ]
    }
   ],
   "source": [
    "print(train_dir)\n",
    "print(val_dir)\n",
    "print(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "76a292e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training File Count: 10754\n",
      "Validation File Count: 3566\n",
      "Test File Size: 3580\n"
     ]
    }
   ],
   "source": [
    "train_files = os.listdir(train_dir)\n",
    "test_files = os.listdir(test_dir)\n",
    "val_files = os.listdir(val_dir)\n",
    "\n",
    "for i in range(0, len(train_files)):\n",
    "    filename = train_files[i]\n",
    "    train_files[i] = os.path.join(train_dir, filename)\n",
    "    \n",
    "for i in range(0, len(test_files)):\n",
    "    filename = test_files[i]\n",
    "    test_files[i] = os.path.join(test_dir, filename)\n",
    "    \n",
    "for i in range(0, len(val_files)):\n",
    "    filename = val_files[i]\n",
    "    val_files[i] = os.path.join(val_dir, filename)\n",
    "\n",
    "#Use same order as folder\n",
    "#random.shuffle(train_files)\n",
    "#random.shuffle(test_files)\n",
    "#random.shuffle(val_files)\n",
    "\n",
    "print(f'Training File Count: {len(train_files)}')\n",
    "print(f'Validation File Count: {len(val_files)}')\n",
    "print(f'Test File Size: {len(test_files)}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c852bc",
   "metadata": {},
   "source": [
    "# My Training Functions, Multi-Model and Single-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b942ab46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files at 100% = 10754\n",
      "Steps per Epoch = 672\n",
      "Validation Steps = 44\n",
      "Epoch 1/100\n",
      "672/672 [==============================] - 380s 557ms/step - loss: 0.4875 - accuracy: 0.8061 - val_loss: 0.3587 - val_accuracy: 0.9473\n",
      "Epoch 2/100\n",
      "672/672 [==============================] - 375s 556ms/step - loss: 0.3092 - accuracy: 0.9640 - val_loss: 0.2919 - val_accuracy: 0.9654\n",
      "Epoch 3/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.2800 - accuracy: 0.9679 - val_loss: 0.2777 - val_accuracy: 0.9716\n",
      "Epoch 4/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.2602 - accuracy: 0.9699 - val_loss: 0.2535 - val_accuracy: 0.9736\n",
      "Epoch 5/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.2440 - accuracy: 0.9711 - val_loss: 0.2451 - val_accuracy: 0.9643\n",
      "Epoch 6/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.2282 - accuracy: 0.9726 - val_loss: 0.2232 - val_accuracy: 0.9723\n",
      "Epoch 7/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.2136 - accuracy: 0.9736 - val_loss: 0.2123 - val_accuracy: 0.9727\n",
      "Epoch 8/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.1994 - accuracy: 0.9748 - val_loss: 0.2026 - val_accuracy: 0.9751\n",
      "Epoch 9/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.1865 - accuracy: 0.9758 - val_loss: 0.1909 - val_accuracy: 0.9707\n",
      "Epoch 10/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.1732 - accuracy: 0.9769 - val_loss: 0.1961 - val_accuracy: 0.9754\n",
      "Epoch 11/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.1616 - accuracy: 0.9778 - val_loss: 0.1737 - val_accuracy: 0.9737\n",
      "Epoch 12/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.1498 - accuracy: 0.9787 - val_loss: 0.1680 - val_accuracy: 0.9750\n",
      "Epoch 13/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.1391 - accuracy: 0.9795 - val_loss: 0.1607 - val_accuracy: 0.9730\n",
      "Epoch 14/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.1288 - accuracy: 0.9802 - val_loss: 0.1533 - val_accuracy: 0.9732\n",
      "Epoch 15/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.1195 - accuracy: 0.9809 - val_loss: 0.1440 - val_accuracy: 0.9761\n",
      "Epoch 16/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.1105 - accuracy: 0.9815 - val_loss: 0.1409 - val_accuracy: 0.9753\n",
      "Epoch 17/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.1032 - accuracy: 0.9818 - val_loss: 0.1374 - val_accuracy: 0.9743\n",
      "Epoch 18/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0954 - accuracy: 0.9823 - val_loss: 0.1260 - val_accuracy: 0.9752\n",
      "Epoch 19/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0885 - accuracy: 0.9826 - val_loss: 0.1227 - val_accuracy: 0.9749\n",
      "Epoch 20/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0824 - accuracy: 0.9829 - val_loss: 0.1255 - val_accuracy: 0.9755\n",
      "Epoch 21/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0769 - accuracy: 0.9831 - val_loss: 0.1161 - val_accuracy: 0.9758\n",
      "Epoch 22/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.0712 - accuracy: 0.9835 - val_loss: 0.1156 - val_accuracy: 0.9756\n",
      "Epoch 23/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0665 - accuracy: 0.9836 - val_loss: 0.1054 - val_accuracy: 0.9754\n",
      "Epoch 24/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0622 - accuracy: 0.9838 - val_loss: 0.1074 - val_accuracy: 0.9757\n",
      "Epoch 25/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0582 - accuracy: 0.9840 - val_loss: 0.1028 - val_accuracy: 0.9757\n",
      "Epoch 26/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0548 - accuracy: 0.9840 - val_loss: 0.0977 - val_accuracy: 0.9765\n",
      "Epoch 27/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0513 - accuracy: 0.9842 - val_loss: 0.0962 - val_accuracy: 0.9750\n",
      "Epoch 28/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0484 - accuracy: 0.9843 - val_loss: 0.0981 - val_accuracy: 0.9766\n",
      "Epoch 29/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0456 - accuracy: 0.9844 - val_loss: 0.0981 - val_accuracy: 0.9767\n",
      "Epoch 30/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0433 - accuracy: 0.9844 - val_loss: 0.0956 - val_accuracy: 0.9765\n",
      "Epoch 31/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.0409 - accuracy: 0.9846 - val_loss: 0.0902 - val_accuracy: 0.9762\n",
      "Epoch 32/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0387 - accuracy: 0.9846 - val_loss: 0.0881 - val_accuracy: 0.9760\n",
      "Epoch 33/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.0369 - accuracy: 0.9847 - val_loss: 0.0923 - val_accuracy: 0.9761\n",
      "Epoch 34/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0353 - accuracy: 0.9848 - val_loss: 0.0818 - val_accuracy: 0.9757\n",
      "Epoch 35/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0337 - accuracy: 0.9848 - val_loss: 0.0834 - val_accuracy: 0.9762\n",
      "Epoch 36/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0323 - accuracy: 0.9849 - val_loss: 0.0791 - val_accuracy: 0.9763\n",
      "Epoch 37/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0312 - accuracy: 0.9849 - val_loss: 0.0829 - val_accuracy: 0.9765\n",
      "Epoch 38/100\n",
      "672/672 [==============================] - 374s 556ms/step - loss: 0.0302 - accuracy: 0.9849 - val_loss: 0.0807 - val_accuracy: 0.9763\n",
      "Epoch 39/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0289 - accuracy: 0.9850 - val_loss: 0.0890 - val_accuracy: 0.9761\n",
      "Epoch 40/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0280 - accuracy: 0.9850 - val_loss: 0.0774 - val_accuracy: 0.9760\n",
      "Epoch 41/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0271 - accuracy: 0.9850 - val_loss: 0.0842 - val_accuracy: 0.9766\n",
      "Epoch 42/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0264 - accuracy: 0.9851 - val_loss: 0.0784 - val_accuracy: 0.9766\n",
      "Epoch 43/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0263 - accuracy: 0.9850 - val_loss: 0.0748 - val_accuracy: 0.9765\n",
      "Epoch 44/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0253 - accuracy: 0.9851 - val_loss: 0.0750 - val_accuracy: 0.9764\n",
      "Epoch 45/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0244 - accuracy: 0.9852 - val_loss: 0.0923 - val_accuracy: 0.9759\n",
      "Epoch 46/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0242 - accuracy: 0.9852 - val_loss: 0.0763 - val_accuracy: 0.9760\n",
      "Epoch 47/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0239 - accuracy: 0.9852 - val_loss: 0.0749 - val_accuracy: 0.9763\n",
      "Epoch 48/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0232 - accuracy: 0.9853 - val_loss: 0.0775 - val_accuracy: 0.9768\n",
      "Epoch 49/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0229 - accuracy: 0.9852 - val_loss: 0.0829 - val_accuracy: 0.9761\n",
      "Epoch 50/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0226 - accuracy: 0.9852 - val_loss: 0.0799 - val_accuracy: 0.9764\n",
      "Epoch 51/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0223 - accuracy: 0.9853 - val_loss: 0.0765 - val_accuracy: 0.9762\n",
      "Epoch 52/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0219 - accuracy: 0.9853 - val_loss: 0.0742 - val_accuracy: 0.9763\n",
      "Epoch 53/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0216 - accuracy: 0.9853 - val_loss: 0.0821 - val_accuracy: 0.9770\n",
      "Epoch 54/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0214 - accuracy: 0.9853 - val_loss: 0.0752 - val_accuracy: 0.9765\n",
      "Epoch 55/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0213 - accuracy: 0.9854 - val_loss: 0.0798 - val_accuracy: 0.9766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0210 - accuracy: 0.9853 - val_loss: 0.0778 - val_accuracy: 0.9770\n",
      "Epoch 57/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0205 - accuracy: 0.9854 - val_loss: 0.0709 - val_accuracy: 0.9768\n",
      "Epoch 58/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0205 - accuracy: 0.9854 - val_loss: 0.0785 - val_accuracy: 0.9765\n",
      "Epoch 59/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0203 - accuracy: 0.9854 - val_loss: 0.0786 - val_accuracy: 0.9768\n",
      "Epoch 60/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0204 - accuracy: 0.9854 - val_loss: 0.0782 - val_accuracy: 0.9764\n",
      "Epoch 61/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0200 - accuracy: 0.9854 - val_loss: 0.0794 - val_accuracy: 0.9763\n",
      "Epoch 62/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0201 - accuracy: 0.9854 - val_loss: 0.0713 - val_accuracy: 0.9766\n",
      "Epoch 63/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0197 - accuracy: 0.9854 - val_loss: 0.0745 - val_accuracy: 0.9761\n",
      "Epoch 64/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0197 - accuracy: 0.9854 - val_loss: 0.0756 - val_accuracy: 0.9763\n",
      "Epoch 65/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0198 - accuracy: 0.9854 - val_loss: 0.0783 - val_accuracy: 0.9763\n",
      "Epoch 66/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0195 - accuracy: 0.9854 - val_loss: 0.0737 - val_accuracy: 0.9768\n",
      "Epoch 67/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0192 - accuracy: 0.9855 - val_loss: 0.0704 - val_accuracy: 0.9764\n",
      "Epoch 68/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0194 - accuracy: 0.9854 - val_loss: 0.0729 - val_accuracy: 0.9757\n",
      "Epoch 69/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0192 - accuracy: 0.9855 - val_loss: 0.0731 - val_accuracy: 0.9756\n",
      "Epoch 70/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0191 - accuracy: 0.9855 - val_loss: 0.0725 - val_accuracy: 0.9765\n",
      "Epoch 71/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0190 - accuracy: 0.9854 - val_loss: 0.0726 - val_accuracy: 0.9764\n",
      "Epoch 72/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0192 - accuracy: 0.9855 - val_loss: 0.0705 - val_accuracy: 0.9768\n",
      "Epoch 73/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0189 - accuracy: 0.9855 - val_loss: 0.0734 - val_accuracy: 0.9767\n",
      "Epoch 74/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0188 - accuracy: 0.9855 - val_loss: 0.0719 - val_accuracy: 0.9761\n",
      "Epoch 75/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0187 - accuracy: 0.9855 - val_loss: 0.0734 - val_accuracy: 0.9767\n",
      "Epoch 76/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0187 - accuracy: 0.9855 - val_loss: 0.0751 - val_accuracy: 0.9768\n",
      "Epoch 77/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0187 - accuracy: 0.9855 - val_loss: 0.0748 - val_accuracy: 0.9768\n",
      "Epoch 78/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0185 - accuracy: 0.9855 - val_loss: 0.0718 - val_accuracy: 0.9762\n",
      "Epoch 79/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0184 - accuracy: 0.9855 - val_loss: 0.0782 - val_accuracy: 0.9765\n",
      "Epoch 80/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0184 - accuracy: 0.9855 - val_loss: 0.0717 - val_accuracy: 0.9764\n",
      "Epoch 81/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0183 - accuracy: 0.9856 - val_loss: 0.0750 - val_accuracy: 0.9766\n",
      "Epoch 82/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0184 - accuracy: 0.9855 - val_loss: 0.0741 - val_accuracy: 0.9768\n",
      "Epoch 83/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0182 - accuracy: 0.9856 - val_loss: 0.0744 - val_accuracy: 0.9763\n",
      "Epoch 84/100\n",
      "672/672 [==============================] - 373s 556ms/step - loss: 0.0182 - accuracy: 0.9855 - val_loss: 0.0779 - val_accuracy: 0.9767\n",
      "Epoch 85/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0180 - accuracy: 0.9856 - val_loss: 0.0751 - val_accuracy: 0.9766\n",
      "Epoch 86/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0181 - accuracy: 0.9856 - val_loss: 0.0767 - val_accuracy: 0.9768\n",
      "Epoch 87/100\n",
      "672/672 [==============================] - 373s 555ms/step - loss: 0.0181 - accuracy: 0.9855 - val_loss: 0.0732 - val_accuracy: 0.9764\n",
      "Restoring model weights from the end of the best epoch.\n",
      "Epoch 00087: early stopping\n",
      "INFO:tensorflow:Assets written to: Trained Models/Model training on ECHONET with Tversky loss/assets\n"
     ]
    }
   ],
   "source": [
    "#Note that I am reading files in the default folder order(alphabetical),\n",
    "#so we assume the files are always in this order.\n",
    "#The reason for this is because the shared model will be used in different\n",
    "#experiments and when the results are plotted, it should\n",
    "#have the same staring point at the specified initial percentage.\n",
    "\n",
    "training_set_size = int(float(INITIAL_PERCENTAGE/100.0) * len(train_files))\n",
    "training_files = train_files[ : training_set_size]\n",
    "\n",
    "model = unet_model()\n",
    "\n",
    "TRAIN_LENGTH = len(training_files)        \n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE       \n",
    "VALIDATION_STEPS = len(val_files) // BATCH_SIZE // VAL_SUBSPLITS\n",
    "\n",
    "print(f'Number of training files at {INITIAL_PERCENTAGE}% = {TRAIN_LENGTH}')\n",
    "print(f'Steps per Epoch = {STEPS_PER_EPOCH}')\n",
    "print(f'Validation Steps = {VALIDATION_STEPS}')\n",
    "\n",
    "#Load Data into tensorflow dataset and map images:\n",
    "val_imgs = tf.data.Dataset.from_tensor_slices(val_files)\n",
    "val_set = val_imgs.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val = val_set.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_data = val.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
    "\n",
    "train_imgs = tf.data.Dataset.from_tensor_slices(training_files)\n",
    "train_set = train_imgs.map(parse_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train = train_set.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_data = (train\n",
    "            .cache()\n",
    "            .shuffle(BUFFER_SIZE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .repeat()\n",
    "            #.map(Augment())  no image augmentation for now\n",
    "            .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "callback_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=EARLY_STOPPING_PATIENCE, verbose=1, restore_best_weights=True)\n",
    "\n",
    "\n",
    "model_history = model.fit(train_data, \n",
    "                          epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH, \n",
    "                          validation_steps=VALIDATION_STEPS, \n",
    "                          validation_data=val_data,\n",
    "                          callbacks=[callback_early_stopping])\n",
    "\n",
    "model.save(f'{SAVE_FOLDER}/{MODEL_NAME}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b0407e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
